==========
Args:Namespace(dataset_source='duke', dataset_target='market1501', batch_size=128, workers=12, height=256, width=128, num_instances=4, arch='resnet50', features=0, dropout=0, lr=0.00035, momentum=0.9, weight_decay=0.0005, warmup_step=10, milestones=[40, 70], resume='', evaluate=False, eval_step=5, rerank=False, epochs=40, iters=200, seed=1, print_freq=100, margin=0.0, data_dir='/hgst/longdn/UCF-main/data', logs_dir='/hgst/longdn/UCF-main/logs/pretrained/duke2market')
==========
=> Dukemtmc loaded
Dataset statistics:
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   702 |    16522 |         8
  query    |   702 |     2228 |         8
  gallery  |  1110 |    17661 |         8
  ----------------------------------------
/hgst/longdn/UCF-main/data/market1501/Market-1501-v15.09.15/bounding_box_test



=> Market1501 loaded
Dataset statistics:
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   751 |    12936 |         6
  query    |   750 |     3368 |         6
  gallery  |   751 |    15913 |         6
  ----------------------------------------
DataParallel(
  (module): ResNet(
    (base): Sequential(
      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    )
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (gap): GeneralizedMeanPoolingP(Parameter containing:
    tensor([3.], device='cuda:0', requires_grad=True), output_size=1)
    (feat_bn): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (classifier0_702): Linear(in_features=2048, out_features=702, bias=False)
  )
)
Epoch: [0][100/200]	Time 2.095 (2.158)	Data 0.000 (0.027)	Loss_ce 6.325 (6.457)	Loss_tr 1.973 (2.631)	Prec 44.53% (18.16%)
Epoch: [0][200/200]	Time 2.139 (2.148)	Data 0.000 (0.029)	Loss_ce 5.916 (6.291)	Loss_tr 1.170 (2.037)	Prec 67.19% (39.91%)
Epoch: [1][100/200]	Time 2.111 (2.121)	Data 0.000 (0.023)	Loss_ce 4.863 (5.442)	Loss_tr 0.745 (0.954)	Prec 77.34% (69.02%)
Epoch: [1][200/200]	Time 2.132 (2.125)	Data 0.000 (0.027)	Loss_ce 3.963 (4.891)	Loss_tr 0.675 (0.828)	Prec 72.66% (74.24%)
Epoch: [2][100/200]	Time 2.083 (2.110)	Data 0.000 (0.020)	Loss_ce 2.820 (3.197)	Loss_tr 0.389 (0.524)	Prec 86.72% (86.09%)
Epoch: [2][200/200]	Time 2.079 (2.117)	Data 0.000 (0.025)	Loss_ce 1.952 (2.723)	Loss_tr 0.310 (0.464)	Prec 94.53% (89.20%)
Epoch: [3][100/200]	Time 2.084 (2.117)	Data 0.000 (0.024)	Loss_ce 1.578 (1.740)	Loss_tr 0.212 (0.327)	Prec 99.22% (94.80%)
Epoch: [3][200/200]	Time 2.085 (2.121)	Data 0.000 (0.027)	Loss_ce 1.339 (1.598)	Loss_tr 0.160 (0.273)	Prec 98.44% (95.85%)
Epoch: [4][100/200]	Time 2.116 (2.116)	Data 0.000 (0.023)	Loss_ce 1.422 (1.350)	Loss_tr 0.321 (0.177)	Prec 95.31% (97.86%)
Epoch: [4][200/200]	Time 2.114 (2.120)	Data 0.000 (0.027)	Loss_ce 1.243 (1.318)	Loss_tr 0.112 (0.164)	Prec 100.00% (98.13%)
Extract Features: [100/156]	Time 0.745 (0.750)	Data 0.000 (0.004)	
Mean AP: 60.9%
CMC Scores:
  top-1          77.6%
  top-5          88.2%
  top-10         91.3%

 * Finished epoch   4  source mAP: 60.9%  best: 60.9% *

Epoch: [5][100/200]	Time 2.124 (2.145)	Data 0.000 (0.028)	Loss_ce 1.238 (1.249)	Loss_tr 0.115 (0.118)	Prec 99.22% (98.70%)
Epoch: [5][200/200]	Time 2.114 (2.151)	Data 0.000 (0.031)	Loss_ce 1.218 (1.233)	Loss_tr 0.124 (0.110)	Prec 98.44% (98.81%)
Epoch: [6][100/200]	Time 2.125 (2.147)	Data 0.000 (0.027)	Loss_ce 1.199 (1.213)	Loss_tr 0.095 (0.100)	Prec 98.44% (99.10%)
Epoch: [6][200/200]	Time 2.116 (2.152)	Data 0.000 (0.031)	Loss_ce 1.143 (1.194)	Loss_tr 0.022 (0.084)	Prec 100.00% (99.22%)
Epoch: [7][100/200]	Time 2.129 (2.152)	Data 0.000 (0.026)	Loss_ce 1.196 (1.176)	Loss_tr 0.080 (0.071)	Prec 98.44% (99.30%)
Epoch: [7][200/200]	Time 2.130 (2.154)	Data 0.000 (0.030)	Loss_ce 1.166 (1.167)	Loss_tr 0.029 (0.066)	Prec 99.22% (99.42%)
Epoch: [8][100/200]	Time 2.126 (2.152)	Data 0.000 (0.026)	Loss_ce 1.202 (1.166)	Loss_tr 0.160 (0.063)	Prec 100.00% (99.43%)
Epoch: [8][200/200]	Time 2.126 (2.158)	Data 0.000 (0.030)	Loss_ce 1.141 (1.163)	Loss_tr 0.048 (0.062)	Prec 100.00% (99.43%)
Epoch: [9][100/200]	Time 2.134 (2.155)	Data 0.000 (0.028)	Loss_ce 1.185 (1.166)	Loss_tr 0.050 (0.068)	Prec 100.00% (99.38%)
Epoch: [9][200/200]	Time 4.032 (2.921)	Data 0.000 (0.031)	Loss_ce 1.183 (1.169)	Loss_tr 0.054 (0.073)	Prec 100.00% (99.44%)
Extract Features: [100/156]	Time 0.804 (1.068)	Data 0.000 (0.004)	
Mean AP: 60.4%
CMC Scores:
  top-1          78.1%
  top-5          88.4%
  top-10         91.6%

 * Finished epoch   9  source mAP: 60.4%  best: 60.9%

Epoch: [10][100/200]	Time 2.197 (3.012)	Data 0.000 (0.027)	Loss_ce 1.135 (1.155)	Loss_tr 0.050 (0.058)	Prec 100.00% (99.51%)
Epoch: [10][200/200]	Time 2.225 (2.621)	Data 0.000 (0.031)	Loss_ce 1.105 (1.136)	Loss_tr 0.008 (0.043)	Prec 100.00% (99.60%)
Epoch: [11][100/200]	Time 2.156 (2.180)	Data 0.000 (0.023)	Loss_ce 1.106 (1.111)	Loss_tr 0.008 (0.030)	Prec 100.00% (99.75%)
Epoch: [11][200/200]	Time 2.142 (2.183)	Data 0.000 (0.028)	Loss_ce 1.101 (1.108)	Loss_tr 0.013 (0.029)	Prec 100.00% (99.71%)
Epoch: [12][100/200]	Time 2.168 (2.179)	Data 0.000 (0.025)	Loss_ce 1.082 (1.093)	Loss_tr 0.005 (0.016)	Prec 100.00% (99.79%)
Epoch: [12][200/200]	Time 2.092 (2.183)	Data 0.000 (0.029)	Loss_ce 1.097 (1.090)	Loss_tr 0.006 (0.016)	Prec 99.22% (99.73%)
Epoch: [13][100/200]	Time 2.166 (2.178)	Data 0.000 (0.025)	Loss_ce 1.070 (1.089)	Loss_tr 0.004 (0.016)	Prec 100.00% (99.77%)
Epoch: [13][200/200]	Time 2.174 (2.186)	Data 0.000 (0.030)	Loss_ce 1.053 (1.079)	Loss_tr 0.003 (0.012)	Prec 100.00% (99.83%)
Epoch: [14][100/200]	Time 2.176 (2.181)	Data 0.000 (0.025)	Loss_ce 1.076 (1.066)	Loss_tr 0.013 (0.008)	Prec 100.00% (99.92%)
Epoch: [14][200/200]	Time 2.185 (2.185)	Data 0.000 (0.030)	Loss_ce 1.082 (1.064)	Loss_tr 0.010 (0.007)	Prec 100.00% (99.91%)
Extract Features: [100/156]	Time 0.776 (0.786)	Data 0.000 (0.006)	
Mean AP: 66.5%
CMC Scores:
  top-1          82.5%
  top-5          90.6%
  top-10         93.2%

 * Finished epoch  14  source mAP: 66.5%  best: 66.5% *

Epoch: [15][100/200]	Time 2.160 (2.176)	Data 0.000 (0.026)	Loss_ce 1.055 (1.060)	Loss_tr 0.017 (0.005)	Prec 100.00% (99.89%)
Epoch: [15][200/200]	Time 2.159 (2.181)	Data 0.000 (0.030)	Loss_ce 1.068 (1.062)	Loss_tr 0.007 (0.006)	Prec 100.00% (99.93%)
Epoch: [16][100/200]	Time 2.185 (2.177)	Data 0.000 (0.024)	Loss_ce 1.131 (1.078)	Loss_tr 0.011 (0.011)	Prec 98.44% (99.82%)
Epoch: [16][200/200]	Time 2.206 (2.182)	Data 0.000 (0.029)	Loss_ce 1.121 (1.097)	Loss_tr 0.020 (0.018)	Prec 100.00% (99.73%)
Epoch: [17][100/200]	Time 2.134 (2.182)	Data 0.000 (0.023)	Loss_ce 1.537 (1.290)	Loss_tr 0.443 (0.182)	Prec 93.75% (98.29%)
Epoch: [17][200/200]	Time 2.127 (2.189)	Data 0.000 (0.030)	Loss_ce 1.178 (1.301)	Loss_tr 0.071 (0.241)	Prec 100.00% (97.84%)
Epoch: [18][100/200]	Time 2.124 (2.175)	Data 0.000 (0.025)	Loss_ce 1.157 (1.193)	Loss_tr 0.108 (0.146)	Prec 100.00% (99.02%)
Epoch: [18][200/200]	Time 2.218 (2.182)	Data 0.000 (0.029)	Loss_ce 1.130 (1.171)	Loss_tr 0.053 (0.116)	Prec 98.44% (99.16%)
Epoch: [19][100/200]	Time 2.200 (2.179)	Data 0.000 (0.025)	Loss_ce 1.099 (1.117)	Loss_tr 0.031 (0.049)	Prec 100.00% (99.54%)
Epoch: [19][200/200]	Time 2.162 (2.184)	Data 0.000 (0.030)	Loss_ce 1.092 (1.107)	Loss_tr 0.027 (0.044)	Prec 100.00% (99.64%)
Extract Features: [100/156]	Time 0.768 (0.778)	Data 0.000 (0.004)	
Mean AP: 66.2%
CMC Scores:
  top-1          81.7%
  top-5          91.1%
  top-10         93.6%

 * Finished epoch  19  source mAP: 66.2%  best: 66.5%

Epoch: [20][100/200]	Time 2.161 (2.176)	Data 0.000 (0.027)	Loss_ce 1.131 (1.078)	Loss_tr 0.011 (0.019)	Prec 98.44% (99.80%)
Epoch: [20][200/200]	Time 2.142 (2.183)	Data 0.000 (0.031)	Loss_ce 1.060 (1.075)	Loss_tr 0.005 (0.017)	Prec 100.00% (99.79%)
Epoch: [21][100/200]	Time 2.182 (2.181)	Data 0.000 (0.027)	Loss_ce 1.056 (1.062)	Loss_tr 0.002 (0.010)	Prec 100.00% (99.90%)
Epoch: [21][200/200]	Time 2.152 (2.182)	Data 0.000 (0.031)	Loss_ce 1.070 (1.063)	Loss_tr 0.009 (0.012)	Prec 100.00% (99.87%)
Epoch: [22][100/200]	Time 2.163 (2.184)	Data 0.000 (0.027)	Loss_ce 1.087 (1.070)	Loss_tr 0.008 (0.013)	Prec 100.00% (99.84%)
Epoch: [22][200/200]	Time 2.169 (2.188)	Data 0.000 (0.031)	Loss_ce 1.085 (1.076)	Loss_tr 0.016 (0.017)	Prec 100.00% (99.84%)
Epoch: [23][100/200]	Time 2.169 (2.186)	Data 0.000 (0.028)	Loss_ce 1.113 (1.120)	Loss_tr 0.087 (0.042)	Prec 100.00% (99.57%)
Epoch: [23][200/200]	Time 2.151 (2.188)	Data 0.000 (0.031)	Loss_ce 1.276 (1.162)	Loss_tr 0.226 (0.087)	Prec 99.22% (99.25%)
Epoch: [24][100/200]	Time 2.153 (2.183)	Data 0.000 (0.028)	Loss_ce 1.205 (1.218)	Loss_tr 0.204 (0.178)	Prec 99.22% (98.70%)
Epoch: [24][200/200]	Time 2.177 (2.186)	Data 0.000 (0.031)	Loss_ce 1.115 (1.191)	Loss_tr 0.159 (0.148)	Prec 100.00% (98.99%)
Extract Features: [100/156]	Time 0.776 (0.782)	Data 0.000 (0.004)	
Mean AP: 63.2%
CMC Scores:
  top-1          79.4%
  top-5          89.5%
  top-10         92.6%

 * Finished epoch  24  source mAP: 63.2%  best: 66.5%

Epoch: [25][100/200]	Time 2.129 (2.177)	Data 0.000 (0.024)	Loss_ce 1.105 (1.114)	Loss_tr 0.043 (0.057)	Prec 100.00% (99.59%)
Epoch: [25][200/200]	Time 2.150 (2.181)	Data 0.000 (0.029)	Loss_ce 1.071 (1.101)	Loss_tr 0.029 (0.046)	Prec 100.00% (99.67%)
Epoch: [26][100/200]	Time 2.129 (2.178)	Data 0.000 (0.025)	Loss_ce 1.078 (1.081)	Loss_tr 0.010 (0.026)	Prec 100.00% (99.73%)
Epoch: [26][200/200]	Time 2.154 (2.186)	Data 0.000 (0.031)	Loss_ce 1.078 (1.078)	Loss_tr 0.035 (0.024)	Prec 99.22% (99.77%)
Epoch: [27][100/200]	Time 2.171 (2.182)	Data 0.000 (0.023)	Loss_ce 1.064 (1.065)	Loss_tr 0.020 (0.012)	Prec 100.00% (99.87%)
Epoch: [27][200/200]	Time 2.162 (2.183)	Data 0.000 (0.029)	Loss_ce 1.045 (1.061)	Loss_tr 0.003 (0.011)	Prec 100.00% (99.86%)
Epoch: [28][100/200]	Time 2.141 (2.180)	Data 0.000 (0.026)	Loss_ce 1.049 (1.057)	Loss_tr 0.003 (0.008)	Prec 100.00% (99.91%)
Epoch: [28][200/200]	Time 2.096 (2.182)	Data 0.000 (0.030)	Loss_ce 1.064 (1.061)	Loss_tr 0.009 (0.010)	Prec 100.00% (99.87%)
Epoch: [29][100/200]	Time 2.202 (2.182)	Data 0.000 (0.026)	Loss_ce 1.177 (1.130)	Loss_tr 0.089 (0.048)	Prec 100.00% (99.51%)
Epoch: [29][200/200]	Time 2.137 (2.187)	Data 0.000 (0.029)	Loss_ce 1.284 (1.198)	Loss_tr 0.290 (0.140)	Prec 98.44% (99.00%)
Extract Features: [100/156]	Time 0.779 (0.786)	Data 0.000 (0.004)	
Mean AP: 55.6%
CMC Scores:
  top-1          74.0%
  top-5          85.1%
  top-10         88.8%

 * Finished epoch  29  source mAP: 55.6%  best: 66.5%

Epoch: [30][100/200]	Time 2.164 (2.181)	Data 0.000 (0.026)	Loss_ce 1.135 (1.191)	Loss_tr 0.090 (0.190)	Prec 100.00% (98.84%)
Epoch: [30][200/200]	Time 2.146 (2.188)	Data 0.000 (0.029)	Loss_ce 1.123 (1.156)	Loss_tr 0.051 (0.137)	Prec 100.00% (99.22%)
Epoch: [31][100/200]	Time 2.147 (2.182)	Data 0.000 (0.023)	Loss_ce 1.149 (1.090)	Loss_tr 0.162 (0.040)	Prec 97.66% (99.72%)
Epoch: [31][200/200]	Time 2.090 (2.186)	Data 0.000 (0.031)	Loss_ce 1.071 (1.085)	Loss_tr 0.013 (0.034)	Prec 100.00% (99.73%)
Epoch: [32][100/200]	Time 2.178 (2.182)	Data 0.000 (0.025)	Loss_ce 1.065 (1.065)	Loss_tr 0.015 (0.018)	Prec 100.00% (99.83%)
Epoch: [32][200/200]	Time 2.183 (2.189)	Data 0.000 (0.031)	Loss_ce 1.071 (1.070)	Loss_tr 0.011 (0.021)	Prec 100.00% (99.83%)
Epoch: [33][100/200]	Time 2.217 (2.185)	Data 0.000 (0.026)	Loss_ce 1.097 (1.083)	Loss_tr 0.031 (0.030)	Prec 100.00% (99.73%)
Epoch: [33][200/200]	Time 2.153 (2.187)	Data 0.000 (0.030)	Loss_ce 1.069 (1.082)	Loss_tr 0.017 (0.028)	Prec 100.00% (99.77%)
Epoch: [34][100/200]	Time 2.162 (2.185)	Data 0.000 (0.026)	Loss_ce 1.070 (1.082)	Loss_tr 0.021 (0.024)	Prec 100.00% (99.78%)
Epoch: [34][200/200]	Time 2.205 (2.188)	Data 0.000 (0.030)	Loss_ce 1.120 (1.083)	Loss_tr 0.040 (0.023)	Prec 100.00% (99.73%)
Extract Features: [100/156]	Time 0.772 (0.788)	Data 0.000 (0.004)	
Mean AP: 63.3%
CMC Scores:
  top-1          80.0%
  top-5          89.4%
  top-10         92.1%

 * Finished epoch  34  source mAP: 63.3%  best: 66.5%

Epoch: [35][100/200]	Time 2.124 (2.175)	Data 0.000 (0.026)	Loss_ce 1.198 (1.155)	Loss_tr 0.246 (0.102)	Prec 97.66% (99.31%)
Epoch: [35][200/200]	Time 2.186 (2.183)	Data 0.000 (0.031)	Loss_ce 1.122 (1.162)	Loss_tr 0.088 (0.123)	Prec 100.00% (99.19%)
Epoch: [36][100/200]	Time 2.137 (2.174)	Data 0.000 (0.024)	Loss_ce 1.162 (1.135)	Loss_tr 0.151 (0.108)	Prec 99.22% (99.43%)
Epoch: [36][200/200]	Time 2.166 (2.185)	Data 0.000 (0.029)	Loss_ce 1.080 (1.116)	Loss_tr 0.048 (0.080)	Prec 100.00% (99.54%)
Epoch: [37][100/200]	Time 2.219 (2.178)	Data 0.000 (0.024)	Loss_ce 1.084 (1.078)	Loss_tr 0.029 (0.027)	Prec 100.00% (99.73%)
Epoch: [37][200/200]	Time 2.174 (2.184)	Data 0.000 (0.030)	Loss_ce 1.058 (1.070)	Loss_tr 0.006 (0.021)	Prec 100.00% (99.81%)
Epoch: [38][100/200]	Time 2.137 (2.178)	Data 0.000 (0.026)	Loss_ce 1.090 (1.057)	Loss_tr 0.042 (0.014)	Prec 99.22% (99.85%)
Epoch: [38][200/200]	Time 2.200 (2.184)	Data 0.000 (0.032)	Loss_ce 1.079 (1.063)	Loss_tr 0.078 (0.018)	Prec 99.22% (99.84%)
Epoch: [39][100/200]	Time 2.198 (2.186)	Data 0.000 (0.027)	Loss_ce 1.039 (1.051)	Loss_tr 0.005 (0.012)	Prec 100.00% (99.93%)
Epoch: [39][200/200]	Time 2.130 (2.189)	Data 0.000 (0.031)	Loss_ce 1.028 (1.043)	Loss_tr 0.004 (0.010)	Prec 100.00% (99.95%)
Extract Features: [100/156]	Time 0.752 (0.777)	Data 0.000 (0.005)	
Mean AP: 67.3%
CMC Scores:
  top-1          82.4%
  top-5          90.3%
  top-10         93.1%

 * Finished epoch  39  source mAP: 67.3%  best: 67.3% *

Test on target domain:
Extract Features: [100/151]	Time 0.789 (0.779)	Data 0.000 (0.007)	
Mean AP: 26.1%
CMC Scores:
  top-1          55.1%
  top-5          71.0%
  top-10         76.7%
