==========
Args:Namespace(dataset_source='market1501', dataset_target='cuhk03', batch_size=32, workers=8, height=256, width=128, num_instances=4, arch='resnet50', features=0, dropout=0, lr=0.00035, momentum=0.9, weight_decay=0.0005, warmup_step=10, milestones=[40, 70], resume='', evaluate=False, eval_step=20, rerank=False, epochs=60, iters=200, seed=1, print_freq=100, margin=0.0, data_dir='/home/dnlong/QLHTTT/UCF-main/data', logs_dir='logs/pretrained/market2cuhk03')
==========
/home/dnlong/QLHTTT/UCF-main/data/market1501/Market-1501-v15.09.15/bounding_box_test
=> Market1501 loaded
Dataset statistics:
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   751 |    12936 |         6
  query    |   750 |     3368 |         6
  gallery  |   751 |    15913 |         6
  ----------------------------------------
/home/dnlong/QLHTTT/UCF-main/data/CUHK03/CUHK03/gallery
=> CUHK03 loaded
Dataset statistics:
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   843 |     8177 |         2
  query    |   440 |     1724 |         2
  gallery  |   440 |     4195 |         2
  ----------------------------------------
DataParallel(
  (module): ResNet(
    (base): Sequential(
      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    )
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (gap): GeneralizedMeanPoolingP(Parameter containing:
    tensor([3.], device='cuda:0', requires_grad=True), output_size=1)
    (feat_bn): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (classifier0_751): Linear(in_features=2048, out_features=751, bias=False)
  )
)
Epoch: [0][100/200]	Time 0.759 (0.971)	Data 0.001 (0.007)	Loss_ce 6.541 (6.616)	Loss_tr 1.236 (2.160)	Prec 12.50% (1.69%)
Epoch: [0][200/200]	Time 0.738 (0.857)	Data 0.000 (0.007)	Loss_ce 6.408 (6.560)	Loss_tr 1.019 (1.701)	Prec 28.12% (5.50%)
Epoch: [1][100/200]	Time 0.736 (0.754)	Data 0.000 (0.008)	Loss_ce 5.989 (6.324)	Loss_tr 0.523 (0.865)	Prec 50.00% (9.66%)
Epoch: [1][200/200]	Time 0.758 (0.750)	Data 0.001 (0.007)	Loss_ce 5.588 (6.134)	Loss_tr 0.583 (0.734)	Prec 50.00% (12.81%)
Epoch: [2][100/200]	Time 0.735 (0.752)	Data 0.000 (0.007)	Loss_ce 5.047 (5.539)	Loss_tr 0.529 (0.445)	Prec 50.00% (15.62%)
Epoch: [2][200/200]	Time 0.748 (0.748)	Data 0.001 (0.007)	Loss_ce 4.523 (5.266)	Loss_tr 0.253 (0.395)	Prec 56.25% (20.97%)
Epoch: [3][100/200]	Time 0.753 (0.750)	Data 0.001 (0.007)	Loss_ce 3.418 (4.530)	Loss_tr 0.045 (0.301)	Prec 75.00% (28.00%)
Epoch: [3][200/200]	Time 0.757 (0.745)	Data 0.001 (0.006)	Loss_ce 3.066 (4.222)	Loss_tr 0.009 (0.278)	Prec 71.88% (34.92%)
Epoch: [4][100/200]	Time 0.728 (0.751)	Data 0.000 (0.007)	Loss_ce 3.008 (3.487)	Loss_tr 0.250 (0.199)	Prec 62.50% (46.69%)
Epoch: [4][200/200]	Time 0.759 (0.746)	Data 0.001 (0.007)	Loss_ce 1.953 (3.191)	Loss_tr 0.016 (0.171)	Prec 87.50% (55.22%)
Epoch: [5][100/200]	Time 0.753 (0.751)	Data 0.001 (0.007)	Loss_ce 2.216 (2.590)	Loss_tr 0.164 (0.158)	Prec 93.75% (67.84%)
Epoch: [5][200/200]	Time 0.725 (0.746)	Data 0.000 (0.007)	Loss_ce 1.860 (2.391)	Loss_tr 0.007 (0.130)	Prec 90.62% (72.48%)
Epoch: [6][100/200]	Time 0.731 (0.751)	Data 0.001 (0.006)	Loss_ce 1.837 (2.042)	Loss_tr 0.015 (0.106)	Prec 90.62% (79.84%)
Epoch: [6][200/200]	Time 0.756 (0.748)	Data 0.001 (0.006)	Loss_ce 1.822 (1.926)	Loss_tr 0.389 (0.098)	Prec 87.50% (82.66%)
Epoch: [7][100/200]	Time 0.759 (0.753)	Data 0.000 (0.006)	Loss_ce 1.783 (1.816)	Loss_tr 0.092 (0.115)	Prec 90.62% (85.25%)
Epoch: [7][200/200]	Time 0.731 (0.748)	Data 0.000 (0.006)	Loss_ce 1.524 (1.747)	Loss_tr 0.030 (0.102)	Prec 90.62% (86.61%)
Epoch: [8][100/200]	Time 0.744 (0.755)	Data 0.001 (0.008)	Loss_ce 1.592 (1.644)	Loss_tr 0.190 (0.090)	Prec 90.62% (89.41%)
Epoch: [8][200/200]	Time 0.751 (0.749)	Data 0.001 (0.008)	Loss_ce 1.547 (1.631)	Loss_tr 0.079 (0.101)	Prec 96.88% (89.89%)
Epoch: [9][100/200]	Time 0.754 (0.754)	Data 0.001 (0.007)	Loss_ce 1.542 (1.559)	Loss_tr 0.048 (0.081)	Prec 90.62% (91.72%)
Epoch: [9][200/200]	Time 0.748 (0.747)	Data 0.001 (0.006)	Loss_ce 1.309 (1.535)	Loss_tr 0.002 (0.073)	Prec 96.88% (92.37%)
Epoch: [10][100/200]	Time 0.738 (0.753)	Data 0.001 (0.009)	Loss_ce 1.277 (1.475)	Loss_tr 0.049 (0.073)	Prec 100.00% (93.34%)
Epoch: [10][200/200]	Time 0.752 (0.747)	Data 0.000 (0.008)	Loss_ce 1.435 (1.472)	Loss_tr 0.012 (0.072)	Prec 93.75% (93.69%)
Epoch: [11][100/200]	Time 0.751 (0.753)	Data 0.000 (0.008)	Loss_ce 1.378 (1.461)	Loss_tr 0.019 (0.100)	Prec 100.00% (94.47%)
Epoch: [11][200/200]	Time 0.751 (0.748)	Data 0.001 (0.008)	Loss_ce 1.374 (1.435)	Loss_tr 0.080 (0.086)	Prec 100.00% (95.00%)
Epoch: [12][100/200]	Time 0.757 (0.754)	Data 0.001 (0.009)	Loss_ce 1.325 (1.369)	Loss_tr 0.030 (0.051)	Prec 100.00% (96.41%)
Epoch: [12][200/200]	Time 0.729 (0.750)	Data 0.001 (0.008)	Loss_ce 1.260 (1.352)	Loss_tr 0.043 (0.057)	Prec 100.00% (96.78%)
Epoch: [13][100/200]	Time 0.492 (0.749)	Data 0.000 (0.008)	Loss_ce 1.388 (1.332)	Loss_tr 0.010 (0.060)	Prec 93.75% (97.22%)
Epoch: [13][200/200]	Time 0.762 (0.746)	Data 0.001 (0.007)	Loss_ce 1.294 (1.320)	Loss_tr 0.002 (0.051)	Prec 100.00% (97.31%)
Epoch: [14][100/200]	Time 0.724 (0.742)	Data 0.001 (0.008)	Loss_ce 1.199 (1.315)	Loss_tr 0.011 (0.048)	Prec 100.00% (98.00%)
Epoch: [14][200/200]	Time 0.749 (0.747)	Data 0.001 (0.007)	Loss_ce 1.460 (1.301)	Loss_tr 0.021 (0.045)	Prec 87.50% (98.02%)
Epoch: [15][100/200]	Time 0.754 (0.746)	Data 0.001 (0.008)	Loss_ce 1.236 (1.262)	Loss_tr 0.033 (0.024)	Prec 100.00% (98.37%)
Epoch: [15][200/200]	Time 0.734 (0.750)	Data 0.000 (0.007)	Loss_ce 1.381 (1.255)	Loss_tr 0.014 (0.025)	Prec 100.00% (98.59%)
Epoch: [16][100/200]	Time 0.749 (0.747)	Data 0.000 (0.008)	Loss_ce 1.182 (1.243)	Loss_tr 0.007 (0.029)	Prec 100.00% (98.84%)
Epoch: [16][200/200]	Time 0.753 (0.749)	Data 0.000 (0.007)	Loss_ce 1.171 (1.241)	Loss_tr 0.025 (0.030)	Prec 100.00% (98.97%)
Epoch: [17][100/200]	Time 0.733 (0.746)	Data 0.001 (0.008)	Loss_ce 1.254 (1.236)	Loss_tr 0.040 (0.025)	Prec 93.75% (99.00%)
Epoch: [17][200/200]	Time 0.756 (0.750)	Data 0.000 (0.007)	Loss_ce 1.364 (1.249)	Loss_tr 0.052 (0.037)	Prec 96.88% (98.72%)
Epoch: [18][100/200]	Time 0.755 (0.751)	Data 0.001 (0.008)	Loss_ce 1.218 (1.342)	Loss_tr 0.015 (0.049)	Prec 100.00% (97.56%)
Epoch: [18][200/200]	Time 0.726 (0.751)	Data 0.001 (0.007)	Loss_ce 1.352 (1.332)	Loss_tr 0.166 (0.062)	Prec 93.75% (97.72%)
Epoch: [19][100/200]	Time 0.759 (0.750)	Data 0.001 (0.009)	Loss_ce 2.181 (1.371)	Loss_tr 0.061 (0.092)	Prec 81.25% (96.81%)
Epoch: [19][200/200]	Time 0.766 (0.753)	Data 0.001 (0.008)	Loss_ce 1.306 (1.352)	Loss_tr 0.012 (0.081)	Prec 96.88% (97.11%)
Extract Features: [100/603]	Time 0.193 (0.172)	Data 0.000 (0.005)	
Extract Features: [200/603]	Time 0.165 (0.172)	Data 0.000 (0.002)	
Extract Features: [300/603]	Time 0.166 (0.166)	Data 0.001 (0.002)	
Extract Features: [400/603]	Time 0.161 (0.165)	Data 0.000 (0.002)	
Extract Features: [500/603]	Time 0.165 (0.165)	Data 0.001 (0.002)	
Extract Features: [600/603]	Time 0.164 (0.165)	Data 0.000 (0.001)	
Mean AP: 66.5%
CMC Scores:
  top-1          85.5%
  top-5          94.6%
  top-10         96.8%

 * Finished epoch  19  source mAP: 66.5%  best: 66.5% *

Epoch: [20][100/200]	Time 0.739 (0.747)	Data 0.001 (0.008)	Loss_ce 1.275 (1.351)	Loss_tr 0.043 (0.079)	Prec 96.88% (96.75%)
Epoch: [20][200/200]	Time 0.751 (0.754)	Data 0.001 (0.008)	Loss_ce 1.197 (1.329)	Loss_tr 0.012 (0.073)	Prec 100.00% (97.20%)
Epoch: [21][100/200]	Time 0.334 (0.418)	Data 0.001 (0.009)	Loss_ce 1.210 (1.294)	Loss_tr 0.027 (0.075)	Prec 100.00% (98.12%)
Epoch: [21][200/200]	Time 0.330 (0.378)	Data 0.000 (0.007)	Loss_ce 1.287 (1.296)	Loss_tr 0.012 (0.066)	Prec 100.00% (97.95%)
Epoch: [22][100/200]	Time 0.331 (0.338)	Data 0.001 (0.008)	Loss_ce 1.382 (1.270)	Loss_tr 0.110 (0.056)	Prec 96.88% (98.44%)
Epoch: [22][200/200]	Time 0.333 (0.338)	Data 0.000 (0.008)	Loss_ce 1.331 (1.264)	Loss_tr 0.011 (0.052)	Prec 93.75% (98.50%)
Epoch: [23][100/200]	Time 0.330 (0.337)	Data 0.000 (0.007)	Loss_ce 1.161 (1.240)	Loss_tr 0.027 (0.039)	Prec 100.00% (98.97%)
Epoch: [23][200/200]	Time 0.329 (0.337)	Data 0.000 (0.008)	Loss_ce 1.191 (1.245)	Loss_tr 0.021 (0.044)	Prec 100.00% (98.61%)
Epoch: [24][100/200]	Time 0.328 (0.339)	Data 0.001 (0.010)	Loss_ce 1.266 (1.299)	Loss_tr 0.053 (0.075)	Prec 100.00% (98.06%)
Epoch: [24][200/200]	Time 0.331 (0.338)	Data 0.000 (0.008)	Loss_ce 1.232 (1.286)	Loss_tr 0.017 (0.061)	Prec 100.00% (98.06%)
Epoch: [25][100/200]	Time 0.393 (0.370)	Data 0.000 (0.009)	Loss_ce 1.228 (1.262)	Loss_tr 0.073 (0.044)	Prec 100.00% (98.37%)
Epoch: [25][200/200]	Time 0.328 (0.355)	Data 0.000 (0.008)	Loss_ce 1.175 (1.249)	Loss_tr 0.038 (0.042)	Prec 100.00% (98.62%)
Epoch: [26][100/200]	Time 0.328 (0.338)	Data 0.000 (0.009)	Loss_ce 1.262 (1.242)	Loss_tr 0.236 (0.044)	Prec 100.00% (98.87%)
Epoch: [26][200/200]	Time 0.328 (0.337)	Data 0.000 (0.008)	Loss_ce 1.332 (1.280)	Loss_tr 0.154 (0.066)	Prec 96.88% (98.16%)
Epoch: [27][100/200]	Time 0.329 (0.337)	Data 0.000 (0.008)	Loss_ce 1.364 (1.337)	Loss_tr 0.121 (0.090)	Prec 90.62% (96.97%)
Epoch: [27][200/200]	Time 0.328 (0.353)	Data 0.001 (0.007)	Loss_ce 1.266 (1.319)	Loss_tr 0.126 (0.097)	Prec 96.88% (97.52%)
Epoch: [28][100/200]	Time 0.327 (0.339)	Data 0.000 (0.009)	Loss_ce 1.210 (1.289)	Loss_tr 0.098 (0.069)	Prec 100.00% (97.78%)
Epoch: [28][200/200]	Time 0.330 (0.337)	Data 0.000 (0.008)	Loss_ce 1.194 (1.274)	Loss_tr 0.024 (0.066)	Prec 100.00% (98.00%)
Epoch: [29][100/200]	Time 0.330 (0.337)	Data 0.000 (0.008)	Loss_ce 1.235 (1.264)	Loss_tr 0.016 (0.075)	Prec 96.88% (98.62%)
Epoch: [29][200/200]	Time 0.328 (0.336)	Data 0.000 (0.007)	Loss_ce 1.303 (1.258)	Loss_tr 0.122 (0.069)	Prec 100.00% (98.53%)
Epoch: [30][100/200]	Time 0.326 (0.337)	Data 0.000 (0.009)	Loss_ce 1.156 (1.238)	Loss_tr 0.012 (0.051)	Prec 100.00% (98.87%)
Epoch: [30][200/200]	Time 0.330 (0.336)	Data 0.001 (0.007)	Loss_ce 1.229 (1.235)	Loss_tr 0.044 (0.054)	Prec 100.00% (98.83%)
Epoch: [31][100/200]	Time 0.327 (0.335)	Data 0.000 (0.007)	Loss_ce 1.222 (1.241)	Loss_tr 0.019 (0.074)	Prec 100.00% (99.12%)
Epoch: [31][200/200]	Time 0.326 (0.336)	Data 0.000 (0.007)	Loss_ce 1.264 (1.253)	Loss_tr 0.017 (0.079)	Prec 96.88% (98.69%)
Epoch: [32][100/200]	Time 0.749 (0.345)	Data 0.001 (0.009)	Loss_ce 1.299 (1.267)	Loss_tr 0.032 (0.063)	Prec 96.88% (98.09%)
Epoch: [32][200/200]	Time 0.754 (0.546)	Data 0.002 (0.009)	Loss_ce 1.255 (1.252)	Loss_tr 0.032 (0.060)	Prec 100.00% (98.45%)
Epoch: [33][100/200]	Time 0.725 (0.746)	Data 0.000 (0.009)	Loss_ce 1.192 (1.241)	Loss_tr 0.021 (0.071)	Prec 100.00% (98.72%)
Epoch: [33][200/200]	Time 0.750 (0.748)	Data 0.001 (0.008)	Loss_ce 1.229 (1.245)	Loss_tr 0.019 (0.068)	Prec 96.88% (98.61%)
Epoch: [34][100/200]	Time 0.749 (0.736)	Data 0.001 (0.008)	Loss_ce 1.166 (1.233)	Loss_tr 0.053 (0.064)	Prec 100.00% (98.87%)
Epoch: [34][200/200]	Time 0.745 (0.743)	Data 0.000 (0.008)	Loss_ce 1.152 (1.226)	Loss_tr 0.040 (0.060)	Prec 100.00% (98.94%)
Epoch: [35][100/200]	Time 0.740 (0.743)	Data 0.001 (0.009)	Loss_ce 1.185 (1.228)	Loss_tr 0.007 (0.058)	Prec 100.00% (98.78%)
Epoch: [35][200/200]	Time 0.758 (0.746)	Data 0.001 (0.008)	Loss_ce 1.252 (1.227)	Loss_tr 0.069 (0.063)	Prec 96.88% (98.81%)
Epoch: [36][100/200]	Time 0.727 (0.740)	Data 0.001 (0.009)	Loss_ce 1.237 (1.266)	Loss_tr 0.150 (0.084)	Prec 96.88% (98.28%)
Epoch: [36][200/200]	Time 0.752 (0.745)	Data 0.000 (0.008)	Loss_ce 1.168 (1.260)	Loss_tr 0.004 (0.083)	Prec 100.00% (98.36%)
Epoch: [37][100/200]	Time 0.742 (0.743)	Data 0.001 (0.008)	Loss_ce 1.213 (1.251)	Loss_tr 0.331 (0.096)	Prec 100.00% (98.44%)
Epoch: [37][200/200]	Time 0.756 (0.747)	Data 0.001 (0.007)	Loss_ce 1.256 (1.243)	Loss_tr 0.095 (0.090)	Prec 100.00% (98.72%)
Epoch: [38][100/200]	Time 0.732 (0.740)	Data 0.000 (0.008)	Loss_ce 1.190 (1.235)	Loss_tr 0.005 (0.074)	Prec 100.00% (98.91%)
Epoch: [38][200/200]	Time 0.746 (0.745)	Data 0.001 (0.008)	Loss_ce 1.141 (1.230)	Loss_tr 0.085 (0.078)	Prec 100.00% (99.00%)
Epoch: [39][100/200]	Time 0.748 (0.744)	Data 0.001 (0.009)	Loss_ce 1.152 (1.188)	Loss_tr 0.023 (0.052)	Prec 100.00% (99.37%)
Epoch: [39][200/200]	Time 0.727 (0.747)	Data 0.001 (0.008)	Loss_ce 1.179 (1.174)	Loss_tr 0.057 (0.043)	Prec 100.00% (99.55%)
Extract Features: [100/603]	Time 0.161 (0.164)	Data 0.000 (0.005)	
Extract Features: [200/603]	Time 0.154 (0.162)	Data 0.000 (0.003)	
Extract Features: [300/603]	Time 0.164 (0.158)	Data 0.001 (0.002)	
Extract Features: [400/603]	Time 0.155 (0.160)	Data 0.000 (0.002)	
Extract Features: [500/603]	Time 0.162 (0.165)	Data 0.000 (0.003)	
Extract Features: [600/603]	Time 0.164 (0.165)	Data 0.000 (0.003)	
Mean AP: 72.7%
CMC Scores:
  top-1          89.3%
  top-5          96.4%
  top-10         98.0%

 * Finished epoch  39  source mAP: 72.7%  best: 72.7% *

Epoch: [40][100/200]	Time 0.734 (0.741)	Data 0.001 (0.011)	Loss_ce 1.125 (1.140)	Loss_tr 0.020 (0.031)	Prec 100.00% (99.87%)
Epoch: [40][200/200]	Time 0.744 (0.747)	Data 0.001 (0.011)	Loss_ce 1.122 (1.138)	Loss_tr 0.006 (0.034)	Prec 100.00% (99.84%)
Epoch: [41][100/200]	Time 0.729 (0.743)	Data 0.001 (0.012)	Loss_ce 1.117 (1.123)	Loss_tr 0.032 (0.032)	Prec 100.00% (99.84%)
Epoch: [41][200/200]	Time 0.725 (0.748)	Data 0.001 (0.012)	Loss_ce 1.151 (1.118)	Loss_tr 0.042 (0.027)	Prec 100.00% (99.91%)
Epoch: [42][100/200]	Time 0.745 (0.742)	Data 0.000 (0.013)	Loss_ce 1.110 (1.115)	Loss_tr 0.058 (0.026)	Prec 100.00% (99.84%)
Epoch: [42][200/200]	Time 0.724 (0.746)	Data 0.000 (0.011)	Loss_ce 1.119 (1.110)	Loss_tr 0.016 (0.022)	Prec 100.00% (99.91%)
Epoch: [43][100/200]	Time 0.748 (0.742)	Data 0.000 (0.010)	Loss_ce 1.087 (1.104)	Loss_tr 0.004 (0.021)	Prec 100.00% (99.94%)
Epoch: [43][200/200]	Time 0.735 (0.747)	Data 0.001 (0.011)	Loss_ce 1.111 (1.101)	Loss_tr 0.007 (0.019)	Prec 100.00% (99.97%)
Epoch: [44][100/200]	Time 0.751 (0.743)	Data 0.001 (0.011)	Loss_ce 1.092 (1.095)	Loss_tr 0.011 (0.020)	Prec 100.00% (99.97%)
Epoch: [44][200/200]	Time 0.727 (0.760)	Data 0.001 (0.013)	Loss_ce 1.092 (1.094)	Loss_tr 0.001 (0.017)	Prec 100.00% (99.98%)
Epoch: [45][100/200]	Time 0.751 (0.742)	Data 0.001 (0.013)	Loss_ce 1.085 (1.092)	Loss_tr 0.004 (0.019)	Prec 100.00% (99.94%)
Epoch: [45][200/200]	Time 0.751 (0.748)	Data 0.000 (0.012)	Loss_ce 1.095 (1.090)	Loss_tr 0.009 (0.018)	Prec 100.00% (99.97%)
Epoch: [46][100/200]	Time 0.725 (0.753)	Data 0.001 (0.012)	Loss_ce 1.077 (1.084)	Loss_tr 0.003 (0.016)	Prec 100.00% (100.00%)
Epoch: [46][200/200]	Time 0.718 (0.749)	Data 0.000 (0.013)	Loss_ce 1.068 (1.084)	Loss_tr 0.002 (0.016)	Prec 100.00% (99.97%)
Epoch: [47][100/200]	Time 0.748 (0.753)	Data 0.000 (0.011)	Loss_ce 1.072 (1.080)	Loss_tr 0.000 (0.016)	Prec 100.00% (100.00%)
Epoch: [47][200/200]	Time 0.747 (0.748)	Data 0.001 (0.011)	Loss_ce 1.073 (1.081)	Loss_tr 0.021 (0.014)	Prec 100.00% (99.98%)
Epoch: [48][100/200]	Time 0.729 (0.753)	Data 0.001 (0.013)	Loss_ce 1.074 (1.080)	Loss_tr 0.005 (0.019)	Prec 100.00% (99.97%)
Epoch: [48][200/200]	Time 0.733 (0.749)	Data 0.000 (0.013)	Loss_ce 1.083 (1.080)	Loss_tr 0.012 (0.016)	Prec 100.00% (99.95%)
Epoch: [49][100/200]	Time 0.728 (0.754)	Data 0.001 (0.014)	Loss_ce 1.073 (1.078)	Loss_tr 0.002 (0.011)	Prec 100.00% (99.91%)
Epoch: [49][200/200]	Time 0.748 (0.748)	Data 0.000 (0.013)	Loss_ce 1.084 (1.077)	Loss_tr 0.001 (0.011)	Prec 100.00% (99.94%)
Epoch: [50][100/200]	Time 0.836 (0.756)	Data 0.108 (0.014)	Loss_ce 1.070 (1.074)	Loss_tr 0.003 (0.012)	Prec 100.00% (100.00%)
Epoch: [50][200/200]	Time 0.747 (0.749)	Data 0.001 (0.013)	Loss_ce 1.064 (1.075)	Loss_tr 0.003 (0.016)	Prec 100.00% (100.00%)
Epoch: [51][100/200]	Time 0.747 (0.749)	Data 0.000 (0.009)	Loss_ce 1.078 (1.072)	Loss_tr 0.011 (0.010)	Prec 100.00% (100.00%)
Epoch: [51][200/200]	Time 0.750 (0.745)	Data 0.001 (0.011)	Loss_ce 1.073 (1.072)	Loss_tr 0.005 (0.011)	Prec 100.00% (100.00%)
Epoch: [52][100/200]	Time 0.722 (0.752)	Data 0.001 (0.011)	Loss_ce 1.078 (1.071)	Loss_tr 0.007 (0.010)	Prec 100.00% (99.97%)
Epoch: [52][200/200]	Time 0.735 (0.749)	Data 0.000 (0.013)	Loss_ce 1.075 (1.070)	Loss_tr 0.012 (0.010)	Prec 100.00% (99.97%)
Epoch: [53][100/200]	Time 0.756 (0.752)	Data 0.001 (0.011)	Loss_ce 1.106 (1.071)	Loss_tr 0.004 (0.017)	Prec 100.00% (99.97%)
Epoch: [53][200/200]	Time 0.755 (0.748)	Data 0.000 (0.013)	Loss_ce 1.079 (1.070)	Loss_tr 0.003 (0.016)	Prec 100.00% (99.97%)
Epoch: [54][100/200]	Time 0.746 (0.753)	Data 0.001 (0.011)	Loss_ce 1.058 (1.068)	Loss_tr 0.001 (0.007)	Prec 100.00% (100.00%)
Epoch: [54][200/200]	Time 0.747 (0.748)	Data 0.001 (0.012)	Loss_ce 1.063 (1.068)	Loss_tr 0.002 (0.007)	Prec 100.00% (100.00%)
Epoch: [55][100/200]	Time 0.735 (0.753)	Data 0.001 (0.014)	Loss_ce 1.084 (1.065)	Loss_tr 0.008 (0.009)	Prec 100.00% (100.00%)
Epoch: [55][200/200]	Time 0.753 (0.753)	Data 0.000 (0.013)	Loss_ce 1.058 (1.065)	Loss_tr 0.006 (0.010)	Prec 100.00% (100.00%)
Epoch: [56][100/200]	Time 0.746 (0.742)	Data 0.001 (0.012)	Loss_ce 1.065 (1.064)	Loss_tr 0.001 (0.007)	Prec 100.00% (100.00%)
Epoch: [56][200/200]	Time 0.739 (0.747)	Data 0.001 (0.012)	Loss_ce 1.074 (1.064)	Loss_tr 0.001 (0.007)	Prec 100.00% (99.98%)
Epoch: [57][100/200]	Time 0.729 (0.743)	Data 0.001 (0.012)	Loss_ce 1.071 (1.064)	Loss_tr 0.003 (0.010)	Prec 100.00% (100.00%)
Epoch: [57][200/200]	Time 0.749 (0.747)	Data 0.001 (0.012)	Loss_ce 1.057 (1.064)	Loss_tr 0.001 (0.010)	Prec 100.00% (99.98%)
Epoch: [58][100/200]	Time 0.728 (0.741)	Data 0.003 (0.010)	Loss_ce 1.067 (1.064)	Loss_tr 0.013 (0.012)	Prec 100.00% (99.97%)
Epoch: [58][200/200]	Time 0.736 (0.746)	Data 0.000 (0.010)	Loss_ce 1.056 (1.064)	Loss_tr 0.006 (0.013)	Prec 100.00% (99.98%)
Epoch: [59][100/200]	Time 0.750 (0.743)	Data 0.000 (0.012)	Loss_ce 1.087 (1.063)	Loss_tr 0.009 (0.005)	Prec 100.00% (99.97%)
Epoch: [59][200/200]	Time 0.750 (0.748)	Data 0.000 (0.012)	Loss_ce 1.055 (1.061)	Loss_tr 0.001 (0.004)	Prec 100.00% (99.97%)
Extract Features: [100/603]	Time 0.161 (0.165)	Data 0.000 (0.005)	
Extract Features: [200/603]	Time 0.075 (0.161)	Data 0.000 (0.003)	
Extract Features: [300/603]	Time 0.167 (0.160)	Data 0.000 (0.002)	
Extract Features: [400/603]	Time 0.164 (0.161)	Data 0.000 (0.002)	
Extract Features: [500/603]	Time 0.163 (0.161)	Data 0.000 (0.001)	
Extract Features: [600/603]	Time 0.164 (0.161)	Data 0.000 (0.001)	
Mean AP: 78.9%
CMC Scores:
  top-1          91.9%
  top-5          97.3%
  top-10         98.5%

 * Finished epoch  59  source mAP: 78.9%  best: 78.9% *

Test on target domain:
Extract Features: [100/185]	Time 0.159 (0.164)	Data 0.000 (0.006)	
Mean AP: 8.5%
CMC Scores:
  top-1          10.6%
  top-5          21.8%
  top-10         29.1%
